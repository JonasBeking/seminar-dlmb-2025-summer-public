{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce891be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchmetrics in /Users/jb/Library/Python/3.9/lib/python/site-packages (1.7.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchmetrics) (2.7.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchmetrics) (0.14.3)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchmetrics) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (58.0.4)\n",
      "Requirement already satisfied: typing_extensions in /Users/jb/Library/Python/3.9/lib/python/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
      "Requirement already satisfied: networkx in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
      "Requirement already satisfied: filelock in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
      "Requirement already satisfied: jinja2 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8334fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[[ 0.2235,  0.5451, -0.2314,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.2235,  0.7961,  0.2235,  ..., -0.2314, -1.0000, -1.0000],\n",
      "         [-1.0000,  0.2235,  0.2235,  ..., -1.0000, -1.0000, -0.2314],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]), tensor([[0., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]]), [1]), 0)\n"
     ]
    }
   ],
   "source": [
    "from amr.dataset import HybridGenomeDataset\n",
    "\n",
    "kleb = \"Klebsiella_pneumoniae_aztreonam\"\n",
    "staphy = \"Staphylococcus_aureus_cefoxitin\"\n",
    "\n",
    "k=6\n",
    "pathogen = staphy\n",
    "genes=[\"pbp4\"]\n",
    "\n",
    "train_dataset = HybridGenomeDataset(\n",
    "    root_dir=\"../data/ds1\",\n",
    "    train_or_test=\"train\",\n",
    "    pathogen=pathogen,\n",
    "    genes=genes,\n",
    "    k=k\n",
    ")\n",
    "\n",
    "test_dataset = HybridGenomeDataset(\n",
    "    root_dir=\"../data/ds1\",\n",
    "    train_or_test=\"test\",\n",
    "    k=k,\n",
    "    pathogen=pathogen,\n",
    "    genes=genes\n",
    ")\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a8a25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def hybrid_collate(batch):\n",
    "    images = []\n",
    "    sequences = []\n",
    "    genes = []\n",
    "    labels = []\n",
    "    \n",
    "    for (img, seq,gene), label in batch:\n",
    "        images.append(img)\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "        genes.append(gene)\n",
    "    \n",
    "    return (torch.stack(images), pad_sequence(sequences, batch_first=True),torch.tensor(genes)), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cff650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def get_train_val_dataloaders(val_split=0.2):\n",
    "    train_split_dataset, val_split_dataset = random_split(\n",
    "        train_dataset, [1 - val_split, val_split]\n",
    "    )\n",
    "    train_loader = DataLoader(train_split_dataset, batch_size=32, shuffle=True, collate_fn=hybrid_collate)\n",
    "    val_loader = DataLoader(val_split_dataset, batch_size=32, shuffle=False, collate_fn=hybrid_collate)\n",
    "    return train_loader,val_loader\n",
    "\n",
    "def get_test_dataloader():\n",
    "    test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,collate_fn=hybrid_collate)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00599bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 5080.6347 | Train Acc: 0.5093 | Val Loss: 4253.8164 | Val Acc: 0.7778\n",
      "Epoch 2/200 | Train Loss: 994.9496 | Train Acc: 0.7130 | Val Loss: 958.4403 | Val Acc: 0.7778\n",
      "Epoch 3/200 | Train Loss: 495.5942 | Train Acc: 0.6574 | Val Loss: 646.2391 | Val Acc: 0.7778\n",
      "Epoch 4/200 | Train Loss: 314.0625 | Train Acc: 0.5000 | Val Loss: 59.2643 | Val Acc: 0.2222\n",
      "Epoch 5/200 | Train Loss: 66.6621 | Train Acc: 0.4722 | Val Loss: 46.6154 | Val Acc: 0.2222\n",
      "Epoch 6/200 | Train Loss: 26.7841 | Train Acc: 0.5463 | Val Loss: 0.6738 | Val Acc: 0.7778\n",
      "Epoch 7/200 | Train Loss: 0.8118 | Train Acc: 0.6944 | Val Loss: 0.6461 | Val Acc: 0.7778\n",
      "Epoch 8/200 | Train Loss: 8.0559 | Train Acc: 0.6667 | Val Loss: 0.6555 | Val Acc: 0.7778\n",
      "Epoch 9/200 | Train Loss: 0.7285 | Train Acc: 0.7130 | Val Loss: 0.6555 | Val Acc: 0.7778\n",
      "Epoch 10/200 | Train Loss: 5.1860 | Train Acc: 0.6296 | Val Loss: 0.6557 | Val Acc: 0.7778\n",
      "Epoch 11/200 | Train Loss: 0.7214 | Train Acc: 0.7037 | Val Loss: 0.6571 | Val Acc: 0.7778\n",
      "Epoch 12/200 | Train Loss: 0.7247 | Train Acc: 0.7130 | Val Loss: 0.6611 | Val Acc: 0.7778\n",
      "Epoch 13/200 | Train Loss: 1.1093 | Train Acc: 0.6759 | Val Loss: 0.6638 | Val Acc: 0.7778\n",
      "Epoch 14/200 | Train Loss: 0.6861 | Train Acc: 0.7130 | Val Loss: 0.6675 | Val Acc: 0.7778\n",
      "Epoch 15/200 | Train Loss: 0.6863 | Train Acc: 0.7130 | Val Loss: 0.6696 | Val Acc: 0.7778\n",
      "Epoch 16/200 | Train Loss: 0.8311 | Train Acc: 0.7130 | Val Loss: 0.6685 | Val Acc: 0.7778\n",
      "Epoch 17/200 | Train Loss: 0.6831 | Train Acc: 0.7130 | Val Loss: 0.6674 | Val Acc: 0.7778\n",
      "Epoch 18/200 | Train Loss: 0.6865 | Train Acc: 0.7130 | Val Loss: 0.6676 | Val Acc: 0.7778\n",
      "Epoch 19/200 | Train Loss: 1.1554 | Train Acc: 0.7222 | Val Loss: 0.6663 | Val Acc: 0.7778\n",
      "Epoch 20/200 | Train Loss: 0.8216 | Train Acc: 0.7037 | Val Loss: 0.6660 | Val Acc: 0.7778\n",
      "Epoch 21/200 | Train Loss: 0.6835 | Train Acc: 0.7130 | Val Loss: 0.6700 | Val Acc: 0.7778\n",
      "Epoch 22/200 | Train Loss: 1.1974 | Train Acc: 0.7130 | Val Loss: 0.6759 | Val Acc: 0.7778\n",
      "Epoch 23/200 | Train Loss: 1.8682 | Train Acc: 0.7037 | Val Loss: 0.6740 | Val Acc: 0.7778\n",
      "Epoch 24/200 | Train Loss: 2.0821 | Train Acc: 0.7130 | Val Loss: 0.6661 | Val Acc: 0.7778\n",
      "Epoch 25/200 | Train Loss: 0.8273 | Train Acc: 0.7130 | Val Loss: 0.6651 | Val Acc: 0.7778\n",
      "Epoch 26/200 | Train Loss: 0.9053 | Train Acc: 0.7037 | Val Loss: 0.6671 | Val Acc: 0.7778\n",
      "Epoch 27/200 | Train Loss: 0.6871 | Train Acc: 0.7130 | Val Loss: 0.6661 | Val Acc: 0.7778\n",
      "Epoch 28/200 | Train Loss: 0.6872 | Train Acc: 0.7130 | Val Loss: 0.6671 | Val Acc: 0.7778\n",
      "Epoch 29/200 | Train Loss: 0.6863 | Train Acc: 0.7130 | Val Loss: 0.6674 | Val Acc: 0.7778\n",
      "Epoch 30/200 | Train Loss: 0.6873 | Train Acc: 0.7130 | Val Loss: 0.6717 | Val Acc: 0.7778\n",
      "Epoch 31/200 | Train Loss: 0.6871 | Train Acc: 0.7130 | Val Loss: 0.6734 | Val Acc: 0.7778\n",
      "Epoch 32/200 | Train Loss: 0.6885 | Train Acc: 0.7130 | Val Loss: 0.6756 | Val Acc: 0.7778\n",
      "Epoch 33/200 | Train Loss: 0.6870 | Train Acc: 0.7130 | Val Loss: 0.6675 | Val Acc: 0.7778\n",
      "Epoch 34/200 | Train Loss: 0.6873 | Train Acc: 0.7130 | Val Loss: 0.6667 | Val Acc: 0.7778\n",
      "Epoch 35/200 | Train Loss: 5.9385 | Train Acc: 0.7037 | Val Loss: 0.6682 | Val Acc: 0.7778\n",
      "Epoch 36/200 | Train Loss: 0.6873 | Train Acc: 0.7130 | Val Loss: 0.6748 | Val Acc: 0.7778\n",
      "Epoch 37/200 | Train Loss: 0.6880 | Train Acc: 0.7130 | Val Loss: 0.6775 | Val Acc: 0.7778\n",
      "Epoch 38/200 | Train Loss: 0.6879 | Train Acc: 0.7130 | Val Loss: 0.6718 | Val Acc: 0.7778\n",
      "Epoch 39/200 | Train Loss: 0.6842 | Train Acc: 0.7130 | Val Loss: 0.6678 | Val Acc: 0.7778\n",
      "Epoch 40/200 | Train Loss: 0.6882 | Train Acc: 0.7130 | Val Loss: 0.6617 | Val Acc: 0.7778\n",
      "Epoch 41/200 | Train Loss: 0.8118 | Train Acc: 0.7130 | Val Loss: 0.6638 | Val Acc: 0.7778\n",
      "Epoch 42/200 | Train Loss: 0.6847 | Train Acc: 0.7130 | Val Loss: 0.6634 | Val Acc: 0.7778\n",
      "Epoch 43/200 | Train Loss: 0.8340 | Train Acc: 0.7037 | Val Loss: 0.6594 | Val Acc: 0.7778\n",
      "Epoch 44/200 | Train Loss: 0.6899 | Train Acc: 0.7130 | Val Loss: 0.6584 | Val Acc: 0.7778\n",
      "Epoch 45/200 | Train Loss: 0.6799 | Train Acc: 0.7130 | Val Loss: 0.6590 | Val Acc: 0.7778\n",
      "Epoch 46/200 | Train Loss: 0.6902 | Train Acc: 0.7130 | Val Loss: 0.6608 | Val Acc: 0.7778\n",
      "Epoch 47/200 | Train Loss: 0.6843 | Train Acc: 0.7130 | Val Loss: 0.6649 | Val Acc: 0.7778\n",
      "Epoch 48/200 | Train Loss: 0.8569 | Train Acc: 0.7130 | Val Loss: 0.6707 | Val Acc: 0.7778\n",
      "Epoch 49/200 | Train Loss: 1.7625 | Train Acc: 0.7130 | Val Loss: 0.6708 | Val Acc: 0.7778\n",
      "Epoch 50/200 | Train Loss: 3.2700 | Train Acc: 0.7222 | Val Loss: 0.6764 | Val Acc: 0.7778\n",
      "Epoch 51/200 | Train Loss: 0.6876 | Train Acc: 0.7130 | Val Loss: 0.6782 | Val Acc: 0.7778\n",
      "Epoch 52/200 | Train Loss: 0.6891 | Train Acc: 0.7130 | Val Loss: 0.6806 | Val Acc: 0.7778\n",
      "Epoch 53/200 | Train Loss: 0.6884 | Train Acc: 0.7130 | Val Loss: 0.6758 | Val Acc: 0.7778\n",
      "Epoch 54/200 | Train Loss: 0.6866 | Train Acc: 0.7130 | Val Loss: 0.6692 | Val Acc: 0.7778\n",
      "Epoch 55/200 | Train Loss: 0.6870 | Train Acc: 0.7130 | Val Loss: 0.6697 | Val Acc: 0.7778\n",
      "Epoch 56/200 | Train Loss: 0.6872 | Train Acc: 0.7130 | Val Loss: 0.6731 | Val Acc: 0.7778\n",
      "Epoch 57/200 | Train Loss: 0.6861 | Train Acc: 0.7130 | Val Loss: 0.6722 | Val Acc: 0.7778\n",
      "Epoch 58/200 | Train Loss: 0.6875 | Train Acc: 0.7130 | Val Loss: 0.6655 | Val Acc: 0.7778\n",
      "Epoch 59/200 | Train Loss: 0.6870 | Train Acc: 0.7130 | Val Loss: 0.6660 | Val Acc: 0.7778\n",
      "Epoch 60/200 | Train Loss: 0.6875 | Train Acc: 0.7130 | Val Loss: 0.6702 | Val Acc: 0.7778\n",
      "Epoch 61/200 | Train Loss: 2.0070 | Train Acc: 0.7037 | Val Loss: 0.6715 | Val Acc: 0.7778\n",
      "Epoch 62/200 | Train Loss: 0.6865 | Train Acc: 0.7130 | Val Loss: 0.6686 | Val Acc: 0.7778\n",
      "Epoch 63/200 | Train Loss: 0.6846 | Train Acc: 0.7130 | Val Loss: 0.6697 | Val Acc: 0.7778\n",
      "Epoch 64/200 | Train Loss: 0.6860 | Train Acc: 0.7130 | Val Loss: 0.6760 | Val Acc: 0.7778\n",
      "Epoch 65/200 | Train Loss: 3.3667 | Train Acc: 0.7130 | Val Loss: 0.6713 | Val Acc: 0.7778\n",
      "Epoch 66/200 | Train Loss: 0.6890 | Train Acc: 0.7130 | Val Loss: 0.6635 | Val Acc: 0.7778\n",
      "Epoch 67/200 | Train Loss: 0.6868 | Train Acc: 0.7130 | Val Loss: 0.6614 | Val Acc: 0.7778\n",
      "Epoch 68/200 | Train Loss: 0.6890 | Train Acc: 0.7130 | Val Loss: 0.6608 | Val Acc: 0.7778\n",
      "Epoch 69/200 | Train Loss: 0.6849 | Train Acc: 0.7130 | Val Loss: 0.6645 | Val Acc: 0.7778\n",
      "Epoch 70/200 | Train Loss: 0.6888 | Train Acc: 0.7130 | Val Loss: 0.6729 | Val Acc: 0.7778\n",
      "Epoch 71/200 | Train Loss: 0.6865 | Train Acc: 0.7130 | Val Loss: 0.6715 | Val Acc: 0.7778\n",
      "Epoch 72/200 | Train Loss: 0.6881 | Train Acc: 0.7130 | Val Loss: 0.6609 | Val Acc: 0.7778\n",
      "Epoch 73/200 | Train Loss: 0.6897 | Train Acc: 0.7130 | Val Loss: 0.6594 | Val Acc: 0.7778\n",
      "Epoch 74/200 | Train Loss: 0.6889 | Train Acc: 0.7130 | Val Loss: 0.6596 | Val Acc: 0.7778\n",
      "Epoch 75/200 | Train Loss: 0.6892 | Train Acc: 0.7130 | Val Loss: 0.6642 | Val Acc: 0.7778\n",
      "Epoch 76/200 | Train Loss: 0.6879 | Train Acc: 0.7130 | Val Loss: 0.6689 | Val Acc: 0.7778\n",
      "Epoch 77/200 | Train Loss: 0.6869 | Train Acc: 0.7130 | Val Loss: 0.6712 | Val Acc: 0.7778\n",
      "Epoch 78/200 | Train Loss: 0.6872 | Train Acc: 0.7130 | Val Loss: 0.6734 | Val Acc: 0.7778\n",
      "Epoch 79/200 | Train Loss: 0.6868 | Train Acc: 0.7130 | Val Loss: 0.6643 | Val Acc: 0.7778\n",
      "Epoch 80/200 | Train Loss: 0.6841 | Train Acc: 0.7130 | Val Loss: 0.6617 | Val Acc: 0.7778\n",
      "Epoch 81/200 | Train Loss: 0.6905 | Train Acc: 0.7130 | Val Loss: 0.6572 | Val Acc: 0.7778\n",
      "Epoch 82/200 | Train Loss: 0.6970 | Train Acc: 0.7037 | Val Loss: 0.6573 | Val Acc: 0.7778\n",
      "Epoch 83/200 | Train Loss: 0.6921 | Train Acc: 0.7130 | Val Loss: 0.6610 | Val Acc: 0.7778\n",
      "Epoch 84/200 | Train Loss: 0.8406 | Train Acc: 0.7037 | Val Loss: 0.6668 | Val Acc: 0.7778\n",
      "Epoch 85/200 | Train Loss: 0.6858 | Train Acc: 0.7130 | Val Loss: 0.6763 | Val Acc: 0.7778\n",
      "Epoch 86/200 | Train Loss: 0.6879 | Train Acc: 0.7130 | Val Loss: 0.6764 | Val Acc: 0.7778\n",
      "Epoch 87/200 | Train Loss: 0.6878 | Train Acc: 0.7130 | Val Loss: 0.6746 | Val Acc: 0.7778\n",
      "Epoch 88/200 | Train Loss: 0.6866 | Train Acc: 0.7130 | Val Loss: 0.6675 | Val Acc: 0.7778\n",
      "Epoch 89/200 | Train Loss: 0.6887 | Train Acc: 0.7130 | Val Loss: 0.6608 | Val Acc: 0.7778\n",
      "Epoch 90/200 | Train Loss: 0.6890 | Train Acc: 0.7130 | Val Loss: 0.6607 | Val Acc: 0.7778\n",
      "Epoch 91/200 | Train Loss: 0.7163 | Train Acc: 0.7037 | Val Loss: 0.6662 | Val Acc: 0.7778\n",
      "Epoch 92/200 | Train Loss: 0.6878 | Train Acc: 0.7130 | Val Loss: 0.6703 | Val Acc: 0.7778\n",
      "Epoch 93/200 | Train Loss: 0.6878 | Train Acc: 0.7130 | Val Loss: 0.6719 | Val Acc: 0.7778\n",
      "Epoch 94/200 | Train Loss: 0.6871 | Train Acc: 0.7130 | Val Loss: 0.6657 | Val Acc: 0.7778\n",
      "Epoch 95/200 | Train Loss: 0.6881 | Train Acc: 0.7130 | Val Loss: 0.6627 | Val Acc: 0.7778\n",
      "Epoch 96/200 | Train Loss: 0.6868 | Train Acc: 0.7130 | Val Loss: 0.6648 | Val Acc: 0.7778\n",
      "Epoch 97/200 | Train Loss: 0.6891 | Train Acc: 0.7130 | Val Loss: 0.6589 | Val Acc: 0.7778\n",
      "Epoch 98/200 | Train Loss: 0.6903 | Train Acc: 0.7130 | Val Loss: 0.6599 | Val Acc: 0.7778\n",
      "Epoch 99/200 | Train Loss: 0.6830 | Train Acc: 0.7130 | Val Loss: 0.6662 | Val Acc: 0.7778\n",
      "Epoch 100/200 | Train Loss: 0.6858 | Train Acc: 0.7130 | Val Loss: 0.6776 | Val Acc: 0.7778\n",
      "Epoch 101/200 | Train Loss: 0.6902 | Train Acc: 0.7130 | Val Loss: 0.6827 | Val Acc: 0.7778\n",
      "Epoch 102/200 | Train Loss: 0.6891 | Train Acc: 0.7130 | Val Loss: 0.6731 | Val Acc: 0.7778\n",
      "Epoch 103/200 | Train Loss: 0.6873 | Train Acc: 0.7130 | Val Loss: 0.6715 | Val Acc: 0.7778\n",
      "Epoch 104/200 | Train Loss: 0.6856 | Train Acc: 0.7130 | Val Loss: 0.6649 | Val Acc: 0.7778\n",
      "Epoch 105/200 | Train Loss: 0.6875 | Train Acc: 0.7130 | Val Loss: 0.6598 | Val Acc: 0.7778\n",
      "Epoch 106/200 | Train Loss: 0.6887 | Train Acc: 0.7130 | Val Loss: 0.6605 | Val Acc: 0.7778\n",
      "Epoch 107/200 | Train Loss: 0.6878 | Train Acc: 0.7130 | Val Loss: 0.6592 | Val Acc: 0.7778\n"
     ]
    }
   ],
   "source": [
    "from net.HybridGenomeNet import HybridGenomeNet\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train_step(dataloader, device, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        images = inputs[0].to(device)\n",
    "        sequences = inputs[1].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model((images, sequences))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    epoch_train_loss = train_loss / train_total\n",
    "    epoch_train_acc = train_correct / train_total\n",
    "    return epoch_train_loss, epoch_train_acc\n",
    "\n",
    "\n",
    "def evaluate_step(dataloader, device, model, criterion):\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            images = inputs[0].to(device)\n",
    "            sequences = inputs[1].to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model((images, sequences))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update confusion matrix\n",
    "            # confmat.update(predicted, labels)\n",
    "    epoch_val_loss = val_loss / val_total\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "\n",
    "    return epoch_val_loss, epoch_val_acc\n",
    "\n",
    "\n",
    "def train_hybrid():\n",
    "    train_loader, val_loader = get_train_val_dataloaders()\n",
    "\n",
    "    device_string = \"cpu\"\n",
    "    if torch.mps.is_available:\n",
    "        device_string = \"mps\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device_string = \"cuda\"\n",
    "\n",
    "    device = torch.device(device_string)\n",
    "    model = HybridGenomeNet().to(device)\n",
    "    class_weights = torch.tensor([2.0, 1.0]).to(device=device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "                                      T_0=30,  # Initial cycle length\n",
    "                                      T_mult=1,  # Cycle length multiplier\n",
    "                                      eta_min=1e-7)  # Minimum LR\n",
    "    \n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    num_epochs = 200\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc = train_step(\n",
    "            train_loader, device, model, criterion, optimizer\n",
    "        )\n",
    "        epoch_val_loss, epoch_val_acc = evaluate_step(\n",
    "            val_loader, device, model, criterion\n",
    "        )\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_train_loss)\n",
    "        history[\"val_loss\"].append(epoch_val_loss)\n",
    "        history[\"train_acc\"].append(epoch_train_acc)\n",
    "        history[\"val_acc\"].append(epoch_val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "            f\"Train Acc: {epoch_train_acc:.4f} | \"\n",
    "            f\"Val Loss: {epoch_val_loss:.4f} | \"\n",
    "            f\"Val Acc: {epoch_val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    test_loader = get_test_dataloader()\n",
    "    test_loss, test_acc = evaluate_step(test_loader, device, model, criterion)\n",
    "    print(\n",
    "        f\"Test Results| \" f\"Test Loss: {test_loss:.4f} | \" f\"Test Acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_hybrid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
