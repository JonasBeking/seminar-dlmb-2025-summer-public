{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T16KLTe-rbiN",
        "outputId": "713dc62f-59b3-4872-ab28-c578eb08c320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: biopython in /Users/jb/Library/Python/3.9/lib/python/site-packages (1.85)\n",
            "Requirement already satisfied: numpy in /Users/jb/Library/Python/3.9/lib/python/site-packages (from biopython) (2.0.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: seaborn in /Users/jb/Library/Python/3.9/lib/python/site-packages (0.13.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from seaborn) (3.9.4)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: pillow>=8 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: matplotlib in /Users/jb/Library/Python/3.9/lib/python/site-packages (3.9.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: numpy>=1.23 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /Users/jb/Library/Python/3.9/lib/python/site-packages (2.7.0)\n",
            "Requirement already satisfied: torchvision in /Users/jb/Library/Python/3.9/lib/python/site-packages (0.22.0)\n",
            "Requirement already satisfied: jinja2 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: filelock in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: numpy in /Users/jb/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fastfcgr==1.0.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (1.0.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sklearn in /Users/jb/Library/Python/3.9/lib/python/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /Users/jb/Library/Python/3.9/lib/python/site-packages (from sklearn) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Users/jb/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (2.0.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fcgr (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for fcgr\u001b[0m\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install biopython\n",
        "%pip install seaborn\n",
        "%pip install matplotlib\n",
        "%pip install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
        "%pip install fastfcgr==1.0.0\n",
        "%pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zLOTRXcMreim"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'seminar-dlmb-2024-winter-public/data/Klebsiella_pneumoniae_aztreonam/train_seq.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mamr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamr_utility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_gene_data, create_gene_datasets\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Creating our dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcreate_gene_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseminar-dlmb-2024-winter-public/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseminar-dlmb-2024-winter-public/data/ds1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Loading the AMR prediction data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_gene_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseminar-dlmb-2024-winter-public/data/ds1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKlebsiella_pneumoniae_aztreonam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgyrA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/seminar-dlmb-2025-summer-public/src/amr/amr_utility.py:47\u001b[0m, in \u001b[0;36mcreate_gene_datasets\u001b[0;34m(prefix_data_folder, output_data_folder)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var_seq_name, var_label_name, var_dest_folder \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_seq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_seq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_label\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)]:\n\u001b[1;32m     46\u001b[0m     gene_sequences \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cur_record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSeqIO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix_data_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar_seq_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfasta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     48\u001b[0m         seq_name_gene \u001b[38;5;241m=\u001b[39m cur_record\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m         seq_name, seq_gene \u001b[38;5;241m=\u001b[39m seq_name_gene\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/Bio/SeqIO/__init__.py:626\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    624\u001b[0m iterator_generator \u001b[38;5;241m=\u001b[39m _FormatToIterator\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterator_generator:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01min\u001b[39;00m AlignIO\u001b[38;5;241m.\u001b[39m_FormatToIterator:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Use Bio.AlignIO to read in the alignments\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (r \u001b[38;5;28;01mfor\u001b[39;00m alignment \u001b[38;5;129;01min\u001b[39;00m AlignIO\u001b[38;5;241m.\u001b[39mparse(handle, \u001b[38;5;28mformat\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m alignment)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/Bio/SeqIO/FastaIO.py:196\u001b[0m, in \u001b[0;36mFastaIterator.__init__\u001b[0;34m(self, source, alphabet)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alphabet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe alphabet argument is no longer supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFasta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/Bio/SeqIO/Interfaces.py:81\u001b[0m, in \u001b[0;36mSequenceIterator.__init__\u001b[0;34m(self, source, alphabet, fmt)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, _PathLikeTypes):\n\u001b[1;32m     80\u001b[0m     mode \u001b[38;5;241m=\u001b[39m modes[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     value \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seminar-dlmb-2024-winter-public/data/Klebsiella_pneumoniae_aztreonam/train_seq.txt'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('seminar-dlmb-2024-winter-public/src/')\n",
        "# from utility.file_utility import FileUtility\n",
        "from amr.amr_utility import load_gene_data, create_gene_datasets\n",
        "\n",
        "# Creating our dataset\n",
        "create_gene_datasets(\"seminar-dlmb-2024-winter-public/\", \"seminar-dlmb-2024-winter-public/data/ds1\")\n",
        "\n",
        "# Loading the AMR prediction data\n",
        "ds = load_gene_data(\"seminar-dlmb-2024-winter-public/data/ds1\", \"Klebsiella_pneumoniae_aztreonam\", \"gyrA\")\n",
        "\n",
        "seq_train = [x[1] for x in ds[\"train\"]]\n",
        "y_train = [x[2] for x in ds[\"train\"]]\n",
        "\n",
        "seq_test = [x[1] for x in ds[\"test\"]]\n",
        "y_test = [x[2] for x in ds[\"test\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI4sMG38rhqD",
        "outputId": "1ab47378-c90e-440f-d188-c243d4f6596c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num of samples in train data: 135\n",
            "Num of samples in test data: 15\n"
          ]
        }
      ],
      "source": [
        "print('Num of samples in train data: {}'.format(len(seq_train)))\n",
        "print('Num of samples in test data: {}'.format(len(seq_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GITz5eTbriHR",
        "outputId": "2a3dcfc8-fcb1-4da8-aa78-80d469f77faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input of the first element:\n",
            "TATTCTTCGTCTTCGGCGATGTCGTCATCGGTATCCGCTTCCGGGGCGATATCGTCATCGCCTTCCGCCGCGCTACCGTCGATGGCATCCAGCTCTTCATCATCCACCGGCTCAGCCACGCGCTGCAGACCTACCACGTTTTCATCTTCCGCGGTGCGGATGAGGATCACGCCCTGGGTGTTACGGCCCACGATGCTCACTTCGGAAACGCGGGTACGCACCAGCGTCCCGGCGTCGGTGATCATCATGATCTGATCGCAGTCATCCACCTGCACGGCGCCGACCACGGAACCGTTACGCTCGGTGACTTTGATCGAAATAACGCCCTGGGTGGCACGCGACTTGGTCGGATACTCCGCCGCCGCGGTACGTTTACCGTAACCGTTTTGCGTCACCGTCAGGATCGCGCCTTCGCCGCGTGGAATAATCAGCGAGACGACGCTGTCGTTTTCCGCAAGCTTAATACCGCGTACGCCGGTGGCGGTACGGCCCATGGCGCGAACGGCGTCTTCCTTGAAGCGTACAACCTTACCGGCCGCGGAGAACAGCATCACTTCATCCTGACCAGAGGTCAGATCGACGCCGATCAGCTCATCGCCTTCGTTCAGGTTGACCGCGATGATACCGGCGGAACGCGGACGGCTGAACTCGGTCAGCGCGGTTTTCTTCACGGTACCGCTGGCGGTGGCCATAAAGACGTTGACGCCCTCTTCGTACTCGCGGACCGGCAGGATGGCGGTGATGCGCTCATCGGCTTCCAGCGGCAGCAGGTTGACGATCGGCCGACCGCGCGCGCCGCGGCTGGCTTCCGGCAGCTGATAGACCTTCATCCAGTACAGACGGCCGCGGCTTGAGAAGCAGAGGATGGTGTCATGGGTGTTGGCCACCAGCAGACGGTCGATAAAGTCTTCTTCTTTAATGCGCGCTGCCGATTTGCCTTTACCGCCACGACGCTGTGCTTCGTAGTCGGTCAGCGGCTGATACTTCACATACCCCTGATGCGACAGGGTGACCACCACATCTTCCTGGTTGATCAGATCTTCGATGTTAATATCAGCGCTGTTGGCGGTGATTTCGGTACGACGTTCGTCGCCGAACTGGTCGCGGATCAGCTCCAGCTCTTCGCGAATCACTTCCATCAGACGGTCGGCGCTGCCGAGAATGTGCAGCAGCTCTGCGATCTGCTCCAGCAGCTCTTTATATTCGTCGAGCAGTTTTTCATGCTCAAGGCCGGTCAGTTTCTGCAGACGCAGATCCAGAATCGCCTGAGCCTGCTGCTCGGTCAGGTAGTATTTACCGTCGCGCACGCCGAACTCTGGCTCCAGCCATTCCGGACGCGCGGCGTCATCCCCGGCGCGCTCCAGCATCGCCGCAACGTTACCGAGATCCCACGCCTGGGCAACCAGCGCCGTTTTCGCTTCTGCCGGGGTCGGCGCGCGACGGATCAGTTCGATAATCGGGTCGATGTTGGCCAGCGCAACGGCCAGCGCTTCGAGGATATGCGCCCGGTCGCGCGCTTTGCGCAGTTCGAAAATCGTACGACGGGTCACCACTTCACGGCGGTGGCGTACAAACGCGGCGATAATGTCCTTCAGGTTCATGATCTTCGGCTGACCATGGTGCAGAGCTACCATGTTGATGCCGAAGGAGACCTGCAGCTGGGTCTGGGAATAGAGGTTGTTAAGCACCACTTCCCCTACCGCATCGCGCTTCACTTCAATCACGATGCGCATCCCGTCTTTATCAGACTCGTCACGCAGCGCGCTGATGCCTTCCACGCGTTTTTCTTTGACCAGCTCGGCGATTTTCTCGATCAGGCGCGCTTTGTTCACCTGATACGGAATTTCGTGCACGATGATGGTTTCGCGGCCGGATTTCGCGTCCACTTCCACTTCCGCGCGCGCGCGAATGTACACTTTACCGCGACCGGTGCGGTAGGCCTCTTCGATGCCGCGACGGCCATTGATAATGGCGGCGGTCGGGAAATCAGGGCCAGGAATATGCGCCATCAGCCCTTCAATGCTGATGTCTTCATCGTCAACATACGCCAGACAGCCGTTAATCACTTCCGTCAGGTTATGCGGCGGTATGTTGGTGGCCATCCCTACGGCGATCCCGGAGGCGCCGTTCACCAGCAGGTTAGGAATTTTGGTCGGCATGACGTCCGGAATACGCTCCGTACCGTCATAGTTGTCGACGAAATCGACCGTCTCTTTTTCAAGATCGGCCATCAGCTCATGAGCGATTTTCGCCAGACGAATTTCGGTATAACGCATCGCCGCGGCGGAGTCGCCGTCGATGGAACCAAAGTTACCCTGGCCGTCCACCAGCATGTAACGCAGCGAGAACGGCTGCGCCATACGCACGATGGTGTTGTATACCGCGATGTCGCCGTGCGGGTGGTATTTACCGATTACGTCACCAACGACACGGGCTGATTTTTTATAGGCTTTGTTCCAGTCATTGCCCAATACGTTCATGGCGTAAAGTACGCGACGGTGTACCGGCTTCAGGCCATCTCGGACATCCGGCAGCGCACGGCCAACAATGACCGACATCGCATAATCCAGATAAGAGTTCTTAAGCTCTTCCTCAATGTTGACCGGTGTAATTTCTCTCGCAAGGTCGCTCATC\n",
            "First label: 1 -> resistant\n"
          ]
        }
      ],
      "source": [
        "print('Input of the first element:\\n{}'.format(seq_train[0]))\n",
        "print('First label: {} -> resistant'.format(y_train[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6Yx-RGFKrku8"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "eN3uFJqlrm8w",
        "outputId": "6300bc9f-2668-449d-90fd-6196519ced5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median and mean sequence length: 2634.0, 2632.296296296296\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "len_train = [len(s) for s in seq_train]\n",
        "sns.histplot(len_train)\n",
        "median = np.median(len_train)\n",
        "mean = np.mean(len_train)\n",
        "print('Median and mean sequence length: {}, {}'.format(median, mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6p9aOHvrpEZ",
        "outputId": "3a2c4bc1-e82f-4f31-9184-4489348dc96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels in training data\n",
            "Non resistant: 11\n",
            "Resistant: 124\n"
          ]
        }
      ],
      "source": [
        "uniq_vals = set(y_train)\n",
        "y_train_int = list(map(int, y_train))\n",
        "print('Labels in training data')\n",
        "print('Non resistant: {}'.format(y_train_int.count(0)))\n",
        "print('Resistant: {}'.format(y_train_int.count(1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq88QNXvrsLv",
        "outputId": "8469db5c-61be-42dc-a903-7eb2be012349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels in test data\n",
            "Non resistant: 4\n",
            "Resistant: 11\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "uniq_vals = set(y_test)\n",
        "y_test_int = list(map(int, y_test))\n",
        "print('Labels in test data')\n",
        "print('Non resistant: {}'.format(y_test_int.count(0)))\n",
        "print('Resistant: {}'.format(y_test_int.count(1)))\n",
        "print(type(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   0 ...   0  71  71]\n",
            " [  0   0   0 ... 142  71 112]\n",
            " [  0   0   0 ...  71  71   0]\n",
            " ...\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "from fastfcgr import FastFCGR \n",
        "\n",
        "def test_fcgr():\n",
        "    fcgr = FastFCGR()\n",
        "# Example usage\n",
        "    test_index = 15\n",
        "    rna_seq = seq_train[test_index]\n",
        "    fcgr.set_sequence(rna_seq)\n",
        "    \n",
        "    fcgr.calculate(scalingFactor=0.5)\n",
        "    fcgr.print_matrix()\n",
        "    fcgr.save_image(\"fcgr_output.png\", d_max=255)\n",
        "\n",
        "def generate_fcgr_images(sequences, kmer_length=6,scaling_factor=0.5):\n",
        "    images = []\n",
        "    for seq in sequences:\n",
        "        fcgr = FastFCGR()\n",
        "        fcgr.initialize(k=kmer_length, isRNA=False)\n",
        "        fcgr.set_sequence(seq)\n",
        "        fcgr.calculate(scalingFactor=scaling_factor)\n",
        "        fcgr_matrix = fcgr.get_matrix\n",
        "\n",
        "        img_data = np.log2(fcgr_matrix + 1)  \n",
        "        img_data = (img_data/img_data.max() * 255).astype(np.uint8)\n",
        "        images.append(img_data)\n",
        "    \n",
        "    return images\n",
        "\n",
        "result = generate_fcgr_images(seq_train)\n",
        "\n",
        "test_index = 80\n",
        "from matplotlib import pyplot as plt\n",
        "plt.title('Is Resistant - ' + str(y_train[test_index]))\n",
        "plt.imshow(result[test_index], interpolation='nearest')\n",
        "plt.show()\n",
        "\n",
        "print(result[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 2000 is out of bounds for axis 1 with size 2000",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[78], line 111\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prepare_dataloaders(\n\u001b[1;32m    100\u001b[0m         train_images\u001b[38;5;241m=\u001b[39mtrain_images,\n\u001b[1;32m    101\u001b[0m         train_sequences\u001b[38;5;241m=\u001b[39mtrain_sequences,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    110\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m get_dataloaders()\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "Cell \u001b[0;32mIn[78], line 38\u001b[0m, in \u001b[0;36mGenomeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[1;32m     37\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 38\u001b[0m sequence \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongest_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     41\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
            "Cell \u001b[0;32mIn[78], line 16\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(dna_sequence, longest_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, base \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dna_sequence):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m bases:\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbases\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoding\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2000 is out of bounds for axis 1 with size 2000"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "def one_hot_encode(dna_sequence,longest_length):\n",
        "    # Define the mapping for one-hot encoding\n",
        "    bases = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "    \n",
        "    # Initialize the encoding matrix\n",
        "    encoding = np.zeros((4, 2000), dtype=np.float32)\n",
        "    \n",
        "    # Perform one-hot encoding\n",
        "    for i, base in enumerate(dna_sequence):\n",
        "        if base in bases:\n",
        "            encoding[bases[base], i] = 1.0\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "class GenomeDataset(Dataset):\n",
        "    def __init__(self, fcgr_images, sequences, labels, transform=None):\n",
        "        self.images = fcgr_images\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "        \n",
        "        longest = max(sequences, key=len)\n",
        "        length = len(longest)\n",
        "        self.longest_sequence = length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        sequence = one_hot_encode(self.sequences[idx],self.longest_sequence)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sequence = torch.from_numpy(sequence)\n",
        "\n",
        "        return image, sequence, label\n",
        "\n",
        "\n",
        "# Training/validation split and data loaders\n",
        "def prepare_dataloaders(\n",
        "    train_images,\n",
        "    train_sequences,\n",
        "    train_labels,\n",
        "    test_images,\n",
        "    test_sequences,\n",
        "    test_labels,\n",
        "    batch_size=32,\n",
        "    val_split=0.2,\n",
        "):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "            transforms.Resize((64, 64), antialias=True),\n",
        "        ]\n",
        "    )\n",
        "    train_dataset = GenomeDataset(\n",
        "        train_images, train_sequences, train_labels, transform\n",
        "    )\n",
        "    test_dataset = GenomeDataset(test_images, test_sequences, test_labels, transform)\n",
        "\n",
        "    train_split_dataset, val_split_dataset = random_split(\n",
        "        train_dataset, [1 - val_split, val_split]\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_split_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def get_dataloaders(kmer_length=6, scaling_factor=0.5, batch_size=32):\n",
        "    train_images = np.array(\n",
        "        generate_fcgr_images(\n",
        "            seq_train, kmer_length=kmer_length, scaling_factor=scaling_factor\n",
        "        )\n",
        "    )\n",
        "    train_sequences = seq_train\n",
        "    train_labels = y_train\n",
        "\n",
        "    test_images = np.array(\n",
        "        generate_fcgr_images(\n",
        "            seq_test, kmer_length=kmer_length, scaling_factor=scaling_factor\n",
        "        )\n",
        "    )\n",
        "    test_sequences = seq_test\n",
        "    test_labels = y_test\n",
        "\n",
        "    return prepare_dataloaders(\n",
        "        train_images=train_images,\n",
        "        train_sequences=train_sequences,\n",
        "        train_labels=train_labels,\n",
        "        test_images=test_images,\n",
        "        test_sequences=test_sequences,\n",
        "        test_labels=test_labels,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader = get_dataloaders()\n",
        "print(next(iter(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "class GenomeResistanceCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # FCGR pathway\n",
        "        self.fcgr_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3)\n",
        "        )\n",
        "        \n",
        "        # Sequence pathway (1D CNN)\n",
        "        self.seq_branch = nn.Sequential(\n",
        "            nn.Conv1d(4, 16, 5),  # One-hot encoded\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "        \n",
        "        # Fusion\n",
        "        self.fc = nn.Linear(61792, 2)\n",
        "\n",
        "    def forward(self, x_fcgr, x_seq):\n",
        "        x1 = self.fcgr_branch(x_fcgr).flatten(1)\n",
        "        x2 = self.seq_branch(x_seq).flatten(1) \n",
        "        x3 = torch.cat([x1,x2],dim=1)\n",
        "        return self.fc(x3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
        "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    # For learning rate adjustment\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        \n",
        "        for inputs, sequences,labels in train_loader:\n",
        "            inputs, sequences, labels = inputs.to(device), sequences.to(device),labels.to(device)\n",
        "            \n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward + backward + optimize\n",
        "            outputs = model(inputs,sequences)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Statistics\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, sequences, labels in val_loader:\n",
        "                inputs, sequences, labels = inputs.to(device),sequences.to(device), labels.to(device)\n",
        "                \n",
        "                outputs = model(inputs,sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                # Statistics\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = train_loss / train_total\n",
        "        epoch_train_acc = train_correct / train_total\n",
        "        epoch_val_loss = val_loss / val_total\n",
        "        epoch_val_acc = val_correct / val_total\n",
        "        \n",
        "        # Update learning rate if needed\n",
        "        scheduler.step(epoch_val_loss)\n",
        "        \n",
        "        # Store history\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "        \n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
        "              f'Train Loss: {epoch_train_loss:.4f} | '\n",
        "              f'Train Acc: {epoch_train_acc:.4f} | '\n",
        "              f'Val Loss: {epoch_val_loss:.4f} | '\n",
        "              f'Val Acc: {epoch_val_acc:.4f}')\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, sequences,labels in test_loader:\n",
        "            inputs, sequences,labels = inputs.to(device),sequences.to(device), labels.to(device)\n",
        "            outputs = model(inputs,sequences)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    \n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds))\n",
        "    \n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "    \n",
        "    return all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500 | Train Loss: 2.4124 | Train Acc: 0.7963 | Val Loss: 1.0727 | Val Acc: 0.9259\n",
            "Epoch 2/500 | Train Loss: 1.7864 | Train Acc: 0.6759 | Val Loss: 2.6641 | Val Acc: 0.9259\n",
            "Epoch 3/500 | Train Loss: 2.6822 | Train Acc: 0.9167 | Val Loss: 3.4682 | Val Acc: 0.9259\n",
            "Epoch 4/500 | Train Loss: 2.4107 | Train Acc: 0.9167 | Val Loss: 2.3260 | Val Acc: 0.9259\n",
            "Epoch 5/500 | Train Loss: 1.2331 | Train Acc: 0.7685 | Val Loss: 2.0018 | Val Acc: 0.9259\n",
            "Epoch 6/500 | Train Loss: 1.0208 | Train Acc: 0.9167 | Val Loss: 1.7610 | Val Acc: 0.9259\n",
            "Epoch 7/500 | Train Loss: 0.8218 | Train Acc: 0.8889 | Val Loss: 1.3949 | Val Acc: 0.9259\n",
            "Epoch 8/500 | Train Loss: 0.6163 | Train Acc: 0.9167 | Val Loss: 1.1497 | Val Acc: 0.9259\n",
            "Epoch 9/500 | Train Loss: 0.5578 | Train Acc: 0.9167 | Val Loss: 1.0069 | Val Acc: 0.9259\n",
            "Epoch 10/500 | Train Loss: 0.4714 | Train Acc: 0.9167 | Val Loss: 0.7089 | Val Acc: 0.9259\n",
            "Epoch 11/500 | Train Loss: 0.3979 | Train Acc: 0.9167 | Val Loss: 0.4838 | Val Acc: 0.9259\n",
            "Epoch 12/500 | Train Loss: 0.3438 | Train Acc: 0.8981 | Val Loss: 0.3236 | Val Acc: 0.9259\n",
            "Epoch 13/500 | Train Loss: 0.4240 | Train Acc: 0.9167 | Val Loss: 0.2476 | Val Acc: 0.9259\n",
            "Epoch 14/500 | Train Loss: 0.3872 | Train Acc: 0.9074 | Val Loss: 0.2911 | Val Acc: 0.9259\n",
            "Epoch 15/500 | Train Loss: 0.3400 | Train Acc: 0.9167 | Val Loss: 0.4524 | Val Acc: 0.9259\n",
            "Epoch 16/500 | Train Loss: 0.4081 | Train Acc: 0.8519 | Val Loss: 0.4942 | Val Acc: 0.9259\n",
            "Epoch 17/500 | Train Loss: 0.3040 | Train Acc: 0.9167 | Val Loss: 0.4733 | Val Acc: 0.9259\n",
            "Epoch 18/500 | Train Loss: 0.3037 | Train Acc: 0.9167 | Val Loss: 0.4999 | Val Acc: 0.8889\n",
            "Epoch 19/500 | Train Loss: 0.3589 | Train Acc: 0.8611 | Val Loss: 0.3676 | Val Acc: 0.9259\n",
            "Epoch 20/500 | Train Loss: 0.4069 | Train Acc: 0.9167 | Val Loss: 0.3326 | Val Acc: 0.9259\n",
            "Epoch 21/500 | Train Loss: 0.3395 | Train Acc: 0.9167 | Val Loss: 0.3196 | Val Acc: 0.9259\n",
            "Epoch 22/500 | Train Loss: 0.3122 | Train Acc: 0.9167 | Val Loss: 0.3247 | Val Acc: 0.9259\n",
            "Epoch 23/500 | Train Loss: 0.2720 | Train Acc: 0.9167 | Val Loss: 0.3546 | Val Acc: 0.9259\n",
            "Epoch 24/500 | Train Loss: 0.2644 | Train Acc: 0.9167 | Val Loss: 0.4126 | Val Acc: 0.9259\n",
            "Epoch 25/500 | Train Loss: 0.3290 | Train Acc: 0.9167 | Val Loss: 0.3565 | Val Acc: 0.9259\n",
            "Epoch 26/500 | Train Loss: 0.2834 | Train Acc: 0.9167 | Val Loss: 0.3275 | Val Acc: 0.9259\n",
            "Epoch 27/500 | Train Loss: 0.2754 | Train Acc: 0.9167 | Val Loss: 0.3205 | Val Acc: 0.9259\n",
            "Epoch 28/500 | Train Loss: 0.2643 | Train Acc: 0.9167 | Val Loss: 0.3657 | Val Acc: 0.9259\n",
            "Epoch 29/500 | Train Loss: 0.3011 | Train Acc: 0.9167 | Val Loss: 0.3563 | Val Acc: 0.9259\n",
            "Epoch 30/500 | Train Loss: 0.2910 | Train Acc: 0.9167 | Val Loss: 0.3531 | Val Acc: 0.9259\n",
            "Epoch 31/500 | Train Loss: 0.2857 | Train Acc: 0.9167 | Val Loss: 0.3440 | Val Acc: 0.9259\n",
            "Epoch 32/500 | Train Loss: 0.2643 | Train Acc: 0.9167 | Val Loss: 0.3559 | Val Acc: 0.9259\n",
            "Epoch 33/500 | Train Loss: 0.2728 | Train Acc: 0.9167 | Val Loss: 0.3724 | Val Acc: 0.9259\n",
            "Epoch 34/500 | Train Loss: 0.2701 | Train Acc: 0.9167 | Val Loss: 0.3524 | Val Acc: 0.9259\n",
            "Epoch 35/500 | Train Loss: 0.2671 | Train Acc: 0.9167 | Val Loss: 0.3389 | Val Acc: 0.9259\n",
            "Epoch 36/500 | Train Loss: 0.2707 | Train Acc: 0.9167 | Val Loss: 0.3376 | Val Acc: 0.9259\n",
            "Epoch 37/500 | Train Loss: 0.2686 | Train Acc: 0.9167 | Val Loss: 0.3358 | Val Acc: 0.9259\n",
            "Epoch 38/500 | Train Loss: 0.2644 | Train Acc: 0.9167 | Val Loss: 0.3385 | Val Acc: 0.9259\n",
            "Epoch 39/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3454 | Val Acc: 0.9259\n",
            "Epoch 40/500 | Train Loss: 0.2660 | Train Acc: 0.9167 | Val Loss: 0.3525 | Val Acc: 0.9259\n",
            "Epoch 41/500 | Train Loss: 0.2666 | Train Acc: 0.9167 | Val Loss: 0.3479 | Val Acc: 0.9259\n",
            "Epoch 42/500 | Train Loss: 0.2636 | Train Acc: 0.9167 | Val Loss: 0.3385 | Val Acc: 0.9259\n",
            "Epoch 43/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3357 | Val Acc: 0.9259\n",
            "Epoch 44/500 | Train Loss: 0.2670 | Train Acc: 0.9167 | Val Loss: 0.3345 | Val Acc: 0.9259\n",
            "Epoch 45/500 | Train Loss: 0.2665 | Train Acc: 0.9167 | Val Loss: 0.3343 | Val Acc: 0.9259\n",
            "Epoch 46/500 | Train Loss: 0.2657 | Train Acc: 0.9167 | Val Loss: 0.3359 | Val Acc: 0.9259\n",
            "Epoch 47/500 | Train Loss: 0.2639 | Train Acc: 0.9167 | Val Loss: 0.3364 | Val Acc: 0.9259\n",
            "Epoch 48/500 | Train Loss: 0.2633 | Train Acc: 0.9167 | Val Loss: 0.3366 | Val Acc: 0.9259\n",
            "Epoch 49/500 | Train Loss: 0.2632 | Train Acc: 0.9167 | Val Loss: 0.3349 | Val Acc: 0.9259\n",
            "Epoch 50/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3340 | Val Acc: 0.9259\n",
            "Epoch 51/500 | Train Loss: 0.2634 | Train Acc: 0.9167 | Val Loss: 0.3334 | Val Acc: 0.9259\n",
            "Epoch 52/500 | Train Loss: 0.2632 | Train Acc: 0.9167 | Val Loss: 0.3318 | Val Acc: 0.9259\n",
            "Epoch 53/500 | Train Loss: 0.2635 | Train Acc: 0.9167 | Val Loss: 0.3303 | Val Acc: 0.9259\n",
            "Epoch 54/500 | Train Loss: 0.2638 | Train Acc: 0.9167 | Val Loss: 0.3284 | Val Acc: 0.9259\n",
            "Epoch 55/500 | Train Loss: 0.2633 | Train Acc: 0.9167 | Val Loss: 0.3267 | Val Acc: 0.9259\n",
            "Epoch 56/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3262 | Val Acc: 0.9259\n",
            "Epoch 57/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3263 | Val Acc: 0.9259\n",
            "Epoch 58/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3260 | Val Acc: 0.9259\n",
            "Epoch 59/500 | Train Loss: 0.2631 | Train Acc: 0.9167 | Val Loss: 0.3260 | Val Acc: 0.9259\n",
            "Epoch 60/500 | Train Loss: 0.2632 | Train Acc: 0.9167 | Val Loss: 0.3266 | Val Acc: 0.9259\n",
            "Epoch 61/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3264 | Val Acc: 0.9259\n",
            "Epoch 62/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3266 | Val Acc: 0.9259\n",
            "Epoch 63/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3271 | Val Acc: 0.9259\n",
            "Epoch 64/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3275 | Val Acc: 0.9259\n",
            "Epoch 65/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3280 | Val Acc: 0.9259\n",
            "Epoch 66/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3282 | Val Acc: 0.9259\n",
            "Epoch 67/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3286 | Val Acc: 0.9259\n",
            "Epoch 68/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3286 | Val Acc: 0.9259\n",
            "Epoch 69/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3286 | Val Acc: 0.9259\n",
            "Epoch 70/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3288 | Val Acc: 0.9259\n",
            "Epoch 71/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3290 | Val Acc: 0.9259\n",
            "Epoch 72/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3290 | Val Acc: 0.9259\n",
            "Epoch 73/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 74/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 75/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 76/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 77/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 78/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3294 | Val Acc: 0.9259\n",
            "Epoch 79/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3295 | Val Acc: 0.9259\n",
            "Epoch 80/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3295 | Val Acc: 0.9259\n",
            "Epoch 81/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3295 | Val Acc: 0.9259\n",
            "Epoch 82/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3294 | Val Acc: 0.9259\n",
            "Epoch 83/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3294 | Val Acc: 0.9259\n",
            "Epoch 84/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3294 | Val Acc: 0.9259\n",
            "Epoch 85/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 86/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 87/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 88/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3293 | Val Acc: 0.9259\n",
            "Epoch 89/500 | Train Loss: 0.2630 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 90/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 91/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 92/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 93/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 94/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 95/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 96/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 97/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 98/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 99/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 100/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 101/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 102/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 103/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 104/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 105/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 106/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 107/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 108/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 109/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 110/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 111/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 112/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 113/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 114/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 115/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 116/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 117/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 118/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 119/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 120/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 121/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 122/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 123/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 124/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 125/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 126/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 127/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 128/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 129/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 130/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 131/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 132/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 133/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 134/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 135/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 136/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 137/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 138/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 139/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 140/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 141/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 142/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 143/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 144/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 145/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 146/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 147/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 148/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 149/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 150/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 151/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 152/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 153/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 154/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 155/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 156/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 157/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 158/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 159/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 160/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 161/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 162/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 163/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 164/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 165/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 166/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 167/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 168/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 169/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 170/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 171/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 172/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 173/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 174/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 175/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 176/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 177/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 178/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 179/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 180/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 181/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 182/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 183/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 184/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 185/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 186/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 187/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 188/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 189/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 190/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 191/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 192/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 193/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 194/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 195/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 196/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 197/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 198/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 199/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 200/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 201/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 202/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 203/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 204/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 205/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 206/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 207/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 208/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 209/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 210/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 211/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 212/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 213/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 214/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 215/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 216/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 217/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 218/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 219/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 220/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 221/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 222/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 223/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 224/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 225/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 226/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 227/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 228/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 229/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 230/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 231/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 232/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 233/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 234/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 235/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 236/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 237/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 238/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 239/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 240/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 241/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 242/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 243/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 244/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 245/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 246/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 247/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 248/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 249/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 250/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 251/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 252/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 253/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 254/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 255/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 256/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 257/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 258/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 259/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 260/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 261/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 262/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 263/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 264/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 265/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 266/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 267/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 268/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 269/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 270/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 271/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 272/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 273/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 274/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 275/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 276/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 277/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 278/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 279/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 280/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 281/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 282/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 283/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 284/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 285/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 286/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 287/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 288/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 289/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 290/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 291/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 292/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 293/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 294/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 295/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 296/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 297/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 298/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 299/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 300/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 301/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 302/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 303/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 304/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 305/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 306/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 307/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 308/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 309/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 310/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 311/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 312/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 313/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 314/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 315/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 316/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 317/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 318/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 319/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 320/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 321/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 322/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 323/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 324/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 325/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 326/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 327/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 328/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 329/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 330/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 331/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 332/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 333/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 334/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 335/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 336/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 337/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 338/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 339/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 340/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 341/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 342/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 343/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 344/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 345/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 346/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 347/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 348/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 349/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 350/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 351/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 352/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 353/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 354/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 355/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 356/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 357/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 358/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 359/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 360/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 361/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 362/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 363/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 364/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 365/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 366/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 367/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 368/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 369/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 370/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 371/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 372/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 373/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 374/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 375/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 376/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 377/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 378/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 379/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 380/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 381/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 382/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 383/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 384/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 385/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 386/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 387/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 388/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 389/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 390/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 391/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 392/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 393/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 394/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 395/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 396/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 397/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 398/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 399/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 400/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 401/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 402/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 403/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 404/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 405/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 406/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 407/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 408/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 409/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 410/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 411/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 412/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 413/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 414/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 415/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 416/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 417/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 418/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 419/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 420/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 421/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 422/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 423/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 424/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 425/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 426/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 427/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 428/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 429/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 430/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 431/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 432/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 433/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 434/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 435/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 436/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 437/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 438/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 439/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 440/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 441/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 442/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 443/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 444/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 445/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 446/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 447/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 448/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 449/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 450/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 451/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 452/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 453/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 454/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3291 | Val Acc: 0.9259\n",
            "Epoch 455/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 456/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 457/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 458/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 459/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 460/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 461/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 462/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 463/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 464/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 465/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 466/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 467/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 468/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 469/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 470/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 471/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 472/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 473/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 474/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 475/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 476/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 477/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 478/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 479/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 480/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 481/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 482/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 483/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 484/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 485/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 486/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 487/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 488/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 489/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 490/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 491/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 492/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 493/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 494/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 495/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 496/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 497/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 498/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 499/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Epoch 500/500 | Train Loss: 0.2629 | Train Acc: 0.9167 | Val Loss: 0.3292 | Val Acc: 0.9259\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.73      1.00      0.85        11\n",
            "\n",
            "    accuracy                           0.73        15\n",
            "   macro avg       0.37      0.50      0.42        15\n",
            "weighted avg       0.54      0.73      0.62        15\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0  4]\n",
            " [ 0 11]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jb/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jb/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jb/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1)],\n",
              " [np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(0),\n",
              "  np.int64(0),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(0),\n",
              "  np.int64(0),\n",
              "  np.int64(1),\n",
              "  np.int64(1),\n",
              "  np.int64(1)])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(5477)\n",
        "data_loader_train,data_loader_val,data_loader_test = get_dataloaders(kmer_length=5,scaling_factor=0.5,batch_size=32)\n",
        "test_batch = next(iter(data_loader_train))\n",
        "model =  GenomeResistanceCNN()\n",
        "\n",
        "trained_model,history = train_model(model,data_loader_train,data_loader_val,num_epochs=500,learning_rate=0.001)\n",
        "evaluate_model(trained_model,data_loader_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO6dOwREOWKQKva2W7E2j0Z",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
